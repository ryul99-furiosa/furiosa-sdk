{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3de48b7",
   "metadata": {},
   "source": [
    "# YOLOX-L 모델을 최적화 옵션을 적용해 컴파일하고 실행하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf4842b",
   "metadata": {},
   "source": [
    "퓨리오사 SDK는 모델을 최적으로 컴파일하고 실행하기 위한 여러 옵션을 제공합니다. 이 문서는 퓨리오사 SDK가 제공하는 옵션을 사용해서 YOLOX-L 모델을 보다 최적으로 컴파일하고 실행하는 예를 보입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bbb4fc",
   "metadata": {},
   "source": [
    "## 1. 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee61ecda",
   "metadata": {},
   "source": [
    "이 문서에 포함된 예제를 실행하려면, 퓨리오사 SDK 필수 리눅스 패키지 최신 버전을 설치하고 파이썬 실행 환경을 구축해야 합니다. 리눅스 패키지를 아직 설치하지 않았거나 파이썬 실행 환경을 구성하지 않았다면, 아래 두 문서를 참고해 준비할 수 있습니다:\n",
    "\n",
    "* [드라이버, 펌웨어, 런타임 설치 가이드](https://furiosa-ai.github.io/docs/latest/ko/software/installation.html)\n",
    "* [Python 실행 환경 구성](https://furiosa-ai.github.io/docs/latest/ko/software/python-sdk.html#python)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfbafa3",
   "metadata": {},
   "source": [
    "필수 리눅스 패키지와 파이썬 실행 환경을 준비하고 나면, 다음 단계로 퓨리오사 SDK 파이썬 패키지와 양자화 도구 추가 패키지 최신 버전을 설치합니다.\n",
    "\n",
    "```console\n",
    "$ pip3 install --upgrade 'furiosa-sdk[quantizer]'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a118baf8",
   "metadata": {},
   "source": [
    "마지막으로, OpenCV에 대한 파이썬 바인딩인 `opencv-python-headless` 패키지가 필요합니다. 아래에서 이미지 파일을 읽어 들이고 전처리를 하기 위해 사용합니다.\n",
    "\n",
    "```console\n",
    "$ pip3 install opencv-python-headless\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c65682",
   "metadata": {},
   "source": [
    "### 1.1 YOLOX-L 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951ad471",
   "metadata": {},
   "source": [
    "YOLOX-L 모델 구현으로는 Megvii 사가 [공개](https://yolox.readthedocs.io/en/latest/demo/onnx_readme.html)한 ONNX 모델 [yolox_l.onnx](https://github.com/Megvii-BaseDetection/YOLOX/releases/download/0.1.1rc0/yolox_l.onnx)을 사용합니다. 해당 모델을 다운로드해서 현재 디렉토리에 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a207ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/Megvii-BaseDetection/YOLOX/releases/download/0.1.1rc0/yolox_l.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1020d011",
   "metadata": {},
   "source": [
    "### 1.2 데이터 집합"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addd508a",
   "metadata": {},
   "source": [
    "양자화 패라미터를 결정하기 위한 캘리브레이션 데이터 집합과 성능을 측정하기 위한 테스트 데이터 집합으로 [COCO 데이터 집합](https://cocodataset.org)을 사용합니다. 2017년도 [검증 데이터 집합](http://images.cocodataset.org/zips/val2017.zip) (1 GB)과 [테스트 데이터 집합](http://images.cocodataset.org/zips/test2017.zip) (6 GB)을 내려 받습니다. 아래 `tree` 명령 출력과 같은 디렉토리 구조를 가지도록, 현재 디렉토리 아래에 `coco` 디렉토리를 만들고 다시 그 `coco` 디렉토리 속으로 내려 받은 데이터 집합 압축 파일들을 풀어 줍니다.\n",
    "\n",
    "```console\n",
    "$ mkdir coco\n",
    "\n",
    "$ wget http://images.cocodataset.org/zips/val2017.zip\n",
    "$ unzip -d coco val2017.zip\n",
    "\n",
    "$ wget http://images.cocodataset.org/zips/test2017.zip\n",
    "$ unzip -d coco test2017.zip\n",
    "\n",
    "$ tree coco\n",
    "coco\n",
    "├── test2017\n",
    "│   ├── 000000000001.jpg\n",
    "│   ├── 000000000016.jpg\n",
    "│   ├── 000000000019.jpg\n",
    "│   ...\n",
    "│   ├── 000000581911.jpg\n",
    "│   └── 000000581918.jpg\n",
    "└── val2017\n",
    "    ├── 000000000139.jpg\n",
    "    ├── 000000000285.jpg\n",
    "    ├── 000000000632.jpg\n",
    "    ...\n",
    "    ├── 000000581615.jpg\n",
    "    └── 000000581781.jpg\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e562275",
   "metadata": {},
   "source": [
    "### 1.3 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae8af1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os; os.environ[\"NPU_COMPLETION_CYCLES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c40a9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libfuriosa_hal.so --- v0.11.0, built @ 43c901f\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from itertools import islice\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import onnx\n",
    "import tqdm\n",
    "\n",
    "from furiosa.runtime.sync import create_runner\n",
    "from furiosa.optimizer import optimize_model\n",
    "from furiosa.quantizer import quantize, Calibrator, CalibrationMethod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d84e50",
   "metadata": {},
   "source": [
    "## 2. YOLOX-L 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36725719",
   "metadata": {},
   "source": [
    "YOLOX-L 모델을 메모리로 읽어 들입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "670e87fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = onnx.load_model(\"yolox_l.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4523cc2",
   "metadata": {},
   "source": [
    "해당 모델은 아래 `preproc` 함수를 사용해 전처리한 이미지 데이터 집합을 사용해 훈련되어 있습니다. `preproc` 전처리 함수가 하는 일을 구체적으로 기술하면, 2번째 줄부터 13번째 줄까지는 이미지 데이터 크기를 모델 입력 크기로 변환합니다. 원본 이미지 가로세로비를 그대로 유지한 채 확대 혹은 축소하고, 여백을 값 114을 가진 픽셀들로 채웁니다. 그 다음, 12번째 줄에서 uint8 값(0 ~ 255)를 float32 값(0 ~ 1)로 정규화합니다. 마지막으로 15번째 줄에서 채널(C) 축을 맨 앞으로 옮깁니다. 즉, HxWxC 형태의 데이터를 CxHxW 형태의 데이터로 변환합니다. 전체적으로, 이미지를 다루는 모델 입력에 대해 자주 사용하는 전처리 유형입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aee6de5",
   "metadata": {},
   "source": [
    "https://github.com/Megvii-BaseDetection/YOLOX/blob/68408b4083f818f50aacc29881e6f97cd19fcef2/yolox/data/data_augment.py#L142-L158"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "18886593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(img, input_size, swap=(2, 0, 1), dtype=np.float32):\n",
    "    if len(img.shape) == 3:  # line 2\n",
    "        padded_img = np.ones((input_size[0], input_size[1], 3), dtype=np.uint8) * 114\n",
    "    else:\n",
    "        padded_img = np.ones(input_size, dtype=np.uint8) * 114\n",
    "\n",
    "    r = min(input_size[0] / img.shape[0], input_size[1] / img.shape[1])\n",
    "    resized_img = cv2.resize(\n",
    "        img,\n",
    "        (int(img.shape[1] * r), int(img.shape[0] * r)),\n",
    "        interpolation=cv2.INTER_LINEAR,\n",
    "    ) / 255.0 # line 12\n",
    "    padded_img[: int(img.shape[0] * r), : int(img.shape[1] * r)] = resized_img  # line 13\n",
    "\n",
    "    if swap:\n",
    "        padded_img = padded_img.transpose(swap)  # line 15\n",
    "    padded_img = np.ascontiguousarray(padded_img, dtype=dtype)\n",
    "    return padded_img, r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead12227",
   "metadata": {},
   "source": [
    "## 3. 캘리브레이션과 양자화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2081f744",
   "metadata": {},
   "source": [
    "양자화 패라미터를 결정하기 위해 사용할 캘리브레이션 데이터 집합을 준비합니다. 이 예에서는 빠른 시연을 위해 임의로 COCO 검증 데이터 집합에 속한 100개의 이미지를 사용합니다. 참고로, MLPerf 벤치마크는 500개 이미지를 캘리브레이션 데이터 집합으로 사용합니다. 훈련할 때 사용한 전처리(`preproc`)와 동일한 전처리를 사용하는 점에 주목합니다. 2번째 줄에서 `[np.newaxis, ...]` 부분은 CxHxW 모양의 데이터를 모델 입력이 요구하는 1xCxHxW 모양의 값으로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "390a8129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calibration_dataset = [\n",
    "    preproc(cv2.imread(image), (640, 640))[0][np.newaxis, ...]  # line 2\n",
    "    for image in islice(glob.iglob(\"coco/val2017/*.jpg\"), 100)\n",
    "]\n",
    "len(calibration_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21006e47",
   "metadata": {},
   "source": [
    "앞에서 준비한 YOLOX-L 모델과 캘리브레이션 데이터 집합을 이용하여 캘리브레이션과 양자화를 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "62d5eef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calibration: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [02:06<00:00,  1.26s/images]\n"
     ]
    }
   ],
   "source": [
    "model = optimize_model(model)\n",
    "\n",
    "calibrator = Calibrator(model, CalibrationMethod.MIN_MAX_ASYM)\n",
    "\n",
    "for calibration_data in tqdm.tqdm(calibration_dataset, desc=\"Calibration\", unit=\"images\", mininterval=0.5):\n",
    "    calibrator.collect_data([[calibration_data]])\n",
    "\n",
    "ranges = calibrator.compute_range()\n",
    "\n",
    "model_quantized = quantize(model, ranges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81a28f9",
   "metadata": {},
   "source": [
    "## 4. 컴파일 최적화 옵션"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510032ec",
   "metadata": {},
   "source": [
    "### 4.1 Model Editor\n",
    "\n",
    "Model Editor는 input과 output을 0 ~ 1로 정규화된 실수가 아닌 0 ~ 255의 정규화 이전의 정수로 처리할 수 있도록 도와주는 객체입니다.\n",
    "이 함수를 사용하게 되면 1세대 NPU상에서 float 와 uint8 사이의 변환하는 과정이 생략되어 추론속도가 빨라집니다.\n",
    "input과 output에 독립적으로 적용할 수 있습니다.\n",
    "\n",
    "주의해야할 점은, `calibrator.collect_data` 함수에 데이터를 넣어줄 때에는 0 ~ 1로 정규화된 실수가 들어가야합니다. 즉, 양자화 과정에서는 model editor 적용과 관계없이 0 ~ 1로 정규화된 실수가 입력되어야 합니다.\n",
    "\n",
    "또한 정확도 이슈가 생길 수 있어 때에 따라 적절하게 적용하여야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c91b73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images\n"
     ]
    }
   ],
   "source": [
    "from furiosa.quantizer import ModelEditor, get_pure_input_names, get_output_names, TensorType\n",
    "\n",
    "# model editor 객체 생성\n",
    "editor = ModelEditor(model)\n",
    "\n",
    "input_names = get_pure_input_names(model)\n",
    "output_names = get_output_names(model)\n",
    "\n",
    "# 모델 수정 진행\n",
    "\n",
    "## 입력들에 대해서 model editor 적용\n",
    "for in_name in input_names:\n",
    "    print(in_name)\n",
    "    editor.convert_input_type(in_name, TensorType.UINT8)\n",
    "\n",
    "## 출력들에 대해서 model editor 적용 (주로 이미지가 출력되는 경우에 적용함)\n",
    "# for out_name in output_names:\n",
    "#     print(out_name)\n",
    "#     editor.convert_output_type(out_name, TensorType.UINT8)\n",
    "\n",
    "# 양자화 적용\n",
    "model_quantized_with_editor = quantize(model, ranges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2819b39",
   "metadata": {},
   "source": [
    "### 4.2 Permute Input\n",
    "아래 코드처럼 설정 `\"permute_input\": [[0, 2, 3, 1]]`을 지정하여, 1xHxWxC 입력을 축 순서 교환없이 곧장 받아들일 수 있도록 모델을 변환합니다.\n",
    "이렇게 컴파일러에게 전처리에 대한 정보를 제공하면, 컴파일러는 위에서 주석 처리한 코드와 동일한 계산 결과를 가지면서도 전처리 다음에 오는 실제 모델 실행 코드와 더 매끄럽게 이어져 전체 실행 시간을 단축시키는 코드를 산출할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "25d2fc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler_config = {\n",
    "    \"permute_input\": \n",
    "        [\n",
    "            [0, 2, 3, 1],\n",
    "        ],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2612fa",
   "metadata": {},
   "source": [
    "## 5. 추론 및 레이턴시(latency) 측정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1b92f4",
   "metadata": {},
   "source": [
    "양자화시킨 모델과 위에서 설명한 컴파일 설정을 사용해 세션을 만듭니다. 생성한 세션을 사용해 테스트 데이터 집합에 대해 추론을 합니다. 캘리브레이션할 때와 마찬가지로 빠른 시연을 위해 전체 40,670개 테스트 데이터 이미지 중 임의로 1000개 이미지를 사용합니다. 그리고, 1000개 이미지를 추론하는데 걸린 총 시간을 측정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6e31ed3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2024-11-05T09:07:50.419711Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_rt_core::driver::event_driven::coord\u001b[0m\u001b[2m:\u001b[0m FuriosaRT (v0.10.3, rev: 394c19392, built at: 2023-11-22T08:53:04Z) bootstrapping ...\n",
      "\u001b[2m2024-11-05T09:07:50.422444Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_rt_core::driver::event_driven::coord\u001b[0m\u001b[2m:\u001b[0m Found furiosa-compiler (v0.10.1, rev: 8b00177, built at: 2024-05-28T06:18:01Z)\n",
      "\u001b[2m2024-11-05T09:07:50.422449Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_rt_core::driver::event_driven::coord\u001b[0m\u001b[2m:\u001b[0m Found libhal (type: warboy, v0.12.0, rev: 56530c0 built at: 2023-11-16T12:34:03Z)\n",
      "\u001b[2m2024-11-05T09:07:50.422452Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_rt_core::driver::event_driven::coord\u001b[0m\u001b[2m:\u001b[0m [Runtime-2] detected 1 NPU device(s):\n",
      "\u001b[2m2024-11-05T09:07:50.447803Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_rt_core::driver::event_driven::coord\u001b[0m\u001b[2m:\u001b[0m - [0] npu:2:0-1 (warboy-b0-2pe, 128dpes, firmware: 1.7.8, e9f371e)\n",
      "\u001b[2m2024-11-05T09:07:50.447895Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_rt_core::driver::event_driven::coord\u001b[0m\u001b[2m:\u001b[0m [Runtime-2] started\n",
      "\u001b[2m2024-11-05T09:07:55.834512Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa::runtime\u001b[0m\u001b[2m:\u001b[0m Saving the compilation log into /home/changmin/.local/state/furiosa/logs/compiler-20241105180755-nrcaic.log\n",
      "\u001b[2m2024-11-05T09:07:55.834613Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_rt_core::driver::event_driven::coord\u001b[0m\u001b[2m:\u001b[0m [Runtime-2] created Sess-24d76a2b using npu:2:0-1\n",
      "\u001b[2m2024-11-05T09:07:55.857873Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_rt_core::driver::event_driven::coord\u001b[0m\u001b[2m:\u001b[0m [Sess-24d76a2b] compiling the model (target: warboy-b0-2pe, 128dpes, size: 216.8 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[2m[1/6]\u001b[0m 🔍   Compiling from onnx to dfg\n",
      "Done in 2.543235s\n",
      "\u001b[1m\u001b[2m[2/6]\u001b[0m 🔍   Compiling from dfg to ldfg\n",
      "Done in 307.2884s\n",
      "\u001b[1m\u001b[2m[3/6]\u001b[0m 🔍   Compiling from ldfg to cdfg\n",
      "Done in 0.018127898s\n",
      "\u001b[1m\u001b[2m[4/6]\u001b[0m 🔍   Compiling from cdfg to gir\n",
      "Done in 0.23304361s\n",
      "\u001b[1m\u001b[2m[5/6]\u001b[0m 🔍   Compiling from gir to lir\n",
      "Done in 343.27084s\n",
      "\u001b[1m\u001b[2m[6/6]\u001b[0m 🔍   Compiling from lir to enf\n",
      "Done in 3.6459002s\n",
      "✨  Finished in 656.99963s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2024-11-05T09:18:56.224048Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_rt_core::driver::event_driven::coord\u001b[0m\u001b[2m:\u001b[0m [Sess-24d76a2b] the model compile is successful (took 660 secs)\n",
      "\u001b[2m2024-11-05T09:18:56.298099Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_rt_core::driver::event_driven::coord\u001b[0m\u001b[2m:\u001b[0m [Runtime-2] created 1 NPU threads on npu:2:0-1 (DRAM: 62.9 MiB/16.0 GiB, SRAM: 32.0 MiB/128.0 MiB)\n",
      "\u001b[2m2024-11-05T09:19:27.888718Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_rt_core::driver::event_driven::coord\u001b[0m\u001b[2m:\u001b[0m [Sess-24d76a2b] terminated\n",
      "\u001b[2m2024-11-05T09:19:27.896885Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_rt_core::npu::raw\u001b[0m\u001b[2m:\u001b[0m NPU (npu:2:0-1) has been closed\n",
      "\u001b[2m2024-11-05T09:19:27.898447Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_rt_core::driver::event_driven::coord\u001b[0m\u001b[2m:\u001b[0m [Runtime-2] stopped\n"
     ]
    }
   ],
   "source": [
    "# inference without optimization\n",
    "total_predictions = 0\n",
    "elapsed_time = 0\n",
    "with create_runner(model_quantized) as runner:\n",
    "    for image in islice(glob.iglob(\"coco/test2017/*.jpg\"), 1000):\n",
    "        inputs = [preproc(cv2.imread(image), (640, 640))[0][np.newaxis, ...]]\n",
    "        start = time.perf_counter_ns()\n",
    "        outputs = runner.run(inputs)\n",
    "        elapsed_time += time.perf_counter_ns() - start\n",
    "        total_predictions += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4e8a921a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2024-11-05T09:52:49.103113Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_rt_core::driver::event_driven::coord\u001b[0m\u001b[2m:\u001b[0m FuriosaRT (v0.10.3, rev: 394c19392, built at: 2023-11-22T08:53:04Z) bootstrapping ...\n",
      "\u001b[2m2024-11-05T09:52:49.109421Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_rt_core::driver::event_driven::coord\u001b[0m\u001b[2m:\u001b[0m Found furiosa-compiler (v0.10.1, rev: 8b00177, built at: 2024-05-28T06:18:01Z)\n",
      "\u001b[2m2024-11-05T09:52:49.109436Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_rt_core::driver::event_driven::coord\u001b[0m\u001b[2m:\u001b[0m Found libhal (type: warboy, v0.12.0, rev: 56530c0 built at: 2023-11-16T12:34:03Z)\n",
      "\u001b[2m2024-11-05T09:52:49.109442Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_rt_core::driver::event_driven::coord\u001b[0m\u001b[2m:\u001b[0m [Runtime-4] detected 1 NPU device(s):\n",
      "\u001b[2m2024-11-05T09:52:49.136188Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_rt_core::driver::event_driven::coord\u001b[0m\u001b[2m:\u001b[0m - [0] npu:7:0-1 (warboy-b0-2pe, 128dpes, firmware: 1.7.8, e9f371e)\n",
      "\u001b[2m2024-11-05T09:52:49.136890Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_rt_core::driver::event_driven::coord\u001b[0m\u001b[2m:\u001b[0m [Runtime-4] started\n",
      "\u001b[2m2024-11-05T09:52:58.781207Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa::runtime\u001b[0m\u001b[2m:\u001b[0m Saving the compilation log into /home/changmin/.local/state/furiosa/logs/compiler-20241105185258-cgxfx9.log\n",
      "\u001b[2m2024-11-05T09:52:58.781413Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_rt_core::driver::event_driven::coord\u001b[0m\u001b[2m:\u001b[0m [Runtime-4] created Sess-3021aa8f using npu:7:0-1\n",
      "\u001b[2m2024-11-05T09:52:58.806321Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_rt_core::driver::event_driven::coord\u001b[0m\u001b[2m:\u001b[0m [Sess-3021aa8f] compiling the model (target: warboy-b0-2pe, 128dpes, size: 216.8 MB)\n",
      "\u001b[2m2024-11-05T09:53:02.401239Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_rt_core::driver::event_driven::coord\u001b[0m\u001b[2m:\u001b[0m [Sess-3021aa8f] the model compile is successful (took 3 secs)\n",
      "\u001b[2m2024-11-05T09:53:02.588552Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_rt_core::driver::event_driven::coord\u001b[0m\u001b[2m:\u001b[0m [Runtime-4] created 1 NPU threads on npu:7:0-1 (DRAM: 65.3 MiB/16.0 GiB, SRAM: 32.0 MiB/128.0 MiB)\n",
      "\u001b[2m2024-11-05T09:53:34.857336Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_rt_core::driver::event_driven::coord\u001b[0m\u001b[2m:\u001b[0m [Sess-3021aa8f] terminated\n",
      "\u001b[2m2024-11-05T09:53:34.871172Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_rt_core::npu::raw\u001b[0m\u001b[2m:\u001b[0m NPU (npu:7:0-1) has been closed\n",
      "\u001b[2m2024-11-05T09:53:34.872637Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_rt_core::driver::event_driven::coord\u001b[0m\u001b[2m:\u001b[0m [Runtime-4] stopped\n"
     ]
    }
   ],
   "source": [
    "# inference with optimization\n",
    "total_predictions_editor = 0\n",
    "elapsed_time_editor = 0\n",
    "with create_runner(model_quantized_with_editor, compiler_config=compiler_config) as runner:\n",
    "    for image in islice(glob.iglob(\"coco/test2017/*.jpg\"), 1000):\n",
    "        inputs = [preproc(cv2.imread(image), (640, 640), None, np.uint8)[0][np.newaxis, ...]]\n",
    "        start = time.perf_counter_ns()\n",
    "        outputs = runner.run(inputs)\n",
    "        elapsed_time_editor += time.perf_counter_ns() - start\n",
    "        total_predictions_editor += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859cbb3a",
   "metadata": {},
   "source": [
    "1000개 이미지를 추론하는데 걸린 시간으로부터 평균 레이턴시(latency)를 계산합니다. 퓨리오사 워보이(리비전 A0)와 인텔 제온 골드 6348 프로세서를 장착한 머신에서 14 ms 대의 평균 레이턴시가 측정되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b566525a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Latency: 24.82279327 ms\n",
      "Average Latency: 21.395099173000002 ms\n"
     ]
    }
   ],
   "source": [
    "latency = elapsed_time / total_predictions\n",
    "print(f\"Average Latency: {latency / 1_000_000} ms\")\n",
    "\n",
    "latency = elapsed_time_editor / total_predictions_editor\n",
    "print(f\"Average Latency: {latency / 1_000_000} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b3829b",
   "metadata": {},
   "source": [
    "(주의: 주피터 노트북 상에서 시간을 측정하면 측정 오차가 상당히 클 수 있습니다. [`nbconvert`](https://nbconvert.readthedocs.io/)를 사용해 주피터 노트북으로부터 파이썬 스크립트를 추출하고, 그 파이썬 스크립트를 실행해 시간을 측정하면 보다 안정적인 측정값을 얻을 수 있습니다.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
